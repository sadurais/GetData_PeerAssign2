<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>GetData_PeerAssign2</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>GetData_PeerAssign2</h1>

<p>Coursera Data Science Speclzn - Getting and Cleaning Data - Peer Assignment 2 </p>

<h2>SYNOPSIS</h2>

<p>The goal of this project is to collect, work with, and clean a data set to prepare a filtered, tidy data that can be used for later analysis. The dataset is about wearable computing. Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data that we have represent data collected from the accelerometers from the Samsung Galaxy S smartphone. We read, process and transform the data into a tidy dataset and store it another file for further analysis.</p>

<h2>APPROACH</h2>

<p>We see the following structure inside out dataset zip file &#39;getdata-projectfiles-UCI HAR Dataset.zip&#39;. </p>

<pre><code class="r">    $ tree &quot;UCI HAR Dataset&quot;
    UCI HAR Dataset
    ├── test/
    │   ├── Inertial Signals/
    │   ├── X_test.txt
    │   ├── subject_test.txt
    │   └── y_test.txt
    ├── train/
    │   ├── Inertial Signals/
    │   ├── X_train.txt
    │   ├── subject_train.txt
    │   └── y_train.txt
    ├── README.txt
    ├── activity_labels.txt
    ├── features.txt
    └── features_info.txt
</code></pre>

<p>We can see that the subdirectories &#39;train&#39; and &#39;test&#39; have similar structure and data files. We focus on the 3 data files &#39;X&#39;, &#39;subject&#39;, and &#39;y&#39;: the &#39;feature vector values&#39;, &#39;the test subject from whom the device is collecting the activity data&#39;, and &#39;the activity&#39; itself respectively. A quick cursory look at these files show us that these files are space-separated, do not have header lines or quoted values or comment lines, making read.table suitable for reading. Also the &#39;X&#39; data is 561 variable (columns) data, the entirety of which will not be useful for us, but only the few variables that have to do &#39;mean&#39; and &#39;standard deviation&#39; (that is, those that have &#39;mean&#39; or &#39;std&#39; in their name, case in-sensitive). 
We also notice that, at the root directory, we have meta-data files such as &#39;activity_labels.txt&#39; and &#39;features.txt&#39; giving meaning to the dataset files inside the &#39;train&#39; and &#39;test&#39; subdirectories. &#39;activity_labels.txt&#39; lists the human understandable labels of the SIX activities: WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING and the features.txt gives the labels for the 561 variables.</p>

<h2>Source Credits</h2>

<p><em>For more and thorough information about the origin/ideas of this dataset, please see</em> <a href="http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones">this link</a></p>

<h2>Transformations done to the data:</h2>

<ul>
<li><em>Subsetting the features</em>: Even though there are 561 variables measured
   from the sensors, we need only those that measure the &#39;mean&#39;
   and &#39;standard deviation&#39;. So we grep for those patterns &#39;mean|std&#39; in
   the names of the features that we obtain from the features.txt file.</li>
<li><em>Attach feature-names to the feature data</em>: The feature names and feature
   data are conveyed in two different files: features.txt and
   {train|test}/X_{train|test}.txt files. We attach the names to the data
   in our data frame using names(df) &lt;- features_names_vect</li>
<li><em>Sanitizing the feature labels/names</em>: The feature names from features.txt
   file have non-word characters such as &#39;-&#39;, &#39;(&#39;, &#39;)&#39; that could cause
   problems with certain R functions. We find&amp;replace such chars with
   a &#39;<em>&#39; using regex:  s/\W+/</em>/g  (global replace a sequence of 1 or more
   non-word characters into a single underscore).So, &ldquo;tBodyAcc-arCoeff()-X,1&rdquo;
   becomes &ldquo;tBodyAcc_arCoeff_X_1&rdquo;</li>
<li><em>Ensuring categorical variables are discrete</em>: Variables with few discrete
   values needs to be treated as factors for our further analysis to work
   nicely. We use &#39;as.factor&#39; to convert these variables (activity &amp; dataset_type)</li>
<li><em>Combine training and test datasets into one</em>: The subdirectories &#39;train&#39;
   and &#39;test&#39; have similar structure. We use modularized code to read them
   alike and combine them into one data frame using:
   df &lt;- training-set&#39;s data
   df &lt;- cbind(df, testing-set&#39;s data)</li>
<li><em>Grouping &amp; Summarizing our data</em>: We first group by test-subject, then the
   activity and finally take the mean of all the other variables (with index
   2:ncol-2). With these variables we create our final tidy dataset. This
   second dataset is stored in a file: getdata_peerassign2_tidy_data.txt
   as space separated values, for quicker reading by read.table</li>
</ul>

<h2>Transformation steps</h2>

<p>First we devlop few handy functions:</p>

<pre><code class="r">#Handy function to construct and return the filepath
filePath &lt;- function(datasetType=&quot;train&quot;, fileNamePrefix=&quot;y&quot;) {
    directory &lt;- &quot;UCI HAR Dataset&quot;
    path &lt;- paste(directory, &quot;/&quot;, datasetType, &quot;/&quot;,
                fileNamePrefix, &quot;_&quot;, datasetType, &quot;.txt&quot;, sep=&quot;&quot;)
    path
}
</code></pre>

<pre><code class="r"># Handy function to keep read.table params, setting varNames
# and subsetting variables(columns) in one place.
readTable &lt;- function(datasetType=&quot;train&quot;, fileNamePrefix=&quot;y&quot;,
                      varIndices=NA, varNames=NA) {
    df = read.table(filePath(datasetType, fileNamePrefix), header=FALSE,
               sep=&quot;&quot;, quote=&quot;&quot;, comment.char=&quot;&quot;,
               nrow=7400, colClasses=c(&quot;numeric&quot;))
    if (!is.na(varIndices)) df &lt;- df[, varIndices]
    if (!is.na(varNames)) names(df) &lt;- varNames
    df
}
</code></pre>

<p>If variable indices are passed, this function would subset the data frame 
to have only those variables(columns).  If variable names are passed,
it assigns them to the variables(columns).</p>

<pre><code class="r"># Similar to readTable, but reads the meta-data files such as
# features.txt and activity_labels.txt rather than a data file.
readMetaTable &lt;- function(fileName) {
    directory &lt;- &quot;UCI HAR Dataset&quot;
    filePath &lt;- paste(directory, &quot;/&quot;, fileName, sep=&quot;&quot;)
    read.table(filePath, header=FALSE, sep=&quot;&quot;, quote=&quot;&quot;,
               comment.char=&quot;&quot;)
}
</code></pre>

<h2>The complete code (with annotations to the RUBRICs) is listed below:</h2>

<pre><code class="r">#
# Getting and Cleaning Data - Peer Assignment 2
#
# 1) Merge the training and test sets to create one data set
# 2) Extract only the measurements on the mean and standard-deviation 
#   for each measurement
# 3) Use descriptive activity names to name the activities in the data set
# 4) Appropriately lable the data set with descriptive variable names
# 5) From the data set in step 4, create a second, independent tidy data set 
#   with average of each variable for each activity for each subject
#

library(dplyr)
library(data.table)


#Handy function to construct and return the filepath
filePath &lt;- function(datasetType=&quot;train&quot;, fileNamePrefix=&quot;y&quot;) {
    directory &lt;- &quot;UCI HAR Dataset&quot;
    path &lt;- paste(directory, &quot;/&quot;, datasetType, &quot;/&quot;,
                fileNamePrefix, &quot;_&quot;, datasetType, &quot;.txt&quot;, sep=&quot;&quot;)
    path
}


# Handy function to keep read.table params, setting varNames
# and subsetting variables(columns) in one place.
readTable &lt;- function(datasetType=&quot;train&quot;, fileNamePrefix=&quot;y&quot;,
                      varIndices=NA, varNames=NA) {
    df = read.table(filePath(datasetType, fileNamePrefix), header=FALSE,
               sep=&quot;&quot;, quote=&quot;&quot;, comment.char=&quot;&quot;,
               nrow=7400, colClasses=c(&quot;numeric&quot;))
    if (!is.na(varIndices)) df &lt;- df[, varIndices]
    if (!is.na(varNames)) names(df) &lt;- varNames
    df
}


# Similar to readTable, but reads the meta-data files such as
# features.txt and activity_labels.txt rather than a data file.
readMetaTable &lt;- function(fileName) {
    directory &lt;- &quot;UCI HAR Dataset&quot;
    filePath &lt;- paste(directory, &quot;/&quot;, fileName, sep=&quot;&quot;)
    read.table(filePath, header=FALSE, sep=&quot;&quot;, quote=&quot;&quot;,
               comment.char=&quot;&quot;)
}


readData &lt;- function() {

    # --First, lets read meta-data about features and activity-labels--
    # Read the meaningful names of feature variables. There are some
    # duplicate names. Fortunately none of them have &#39;mean&#39; or &#39;std&#39; in
    # their name that we are interested in. So, we can safely ignore them
    # *according to a community TA*.
    feature_names &lt;- readMetaTable(&quot;features.txt&quot;)

    # *** RUBRIC #2: EXTRACT ONLY MEAN &amp; STDDEV FOR EACH MEASUREMENT ***
    wanted_features_idx &lt;- grep(&quot;mean|std&quot;, feature_names$V2, ignore.case = TRUE)
    wanted_features &lt;- feature_names$V2[wanted_features_idx]

    # *** RUBRIC #4: APPROPRIATELY NAME THE DATASET WITH DESCRIPTIVE VAR NAMES ***
    # Non-word characters such as &#39;-&#39;, &#39;(&#39;, &#39;)&#39; can create problems in R
    # as variable-names. So, lets replace those chars with &#39;__&#39;
    wanted_features &lt;- gsub(&quot;\\W+&quot;, &quot;_&quot;, wanted_features)

    # *** RUBRIC #3: USE DESCRIPTIVE ACTIVITY NAMES ***
    # Read the discrete labels of outcome (aka &quot;y&quot; or &quot;activity&quot;)
    activity_names &lt;- readMetaTable(&quot;activity_labels.txt&quot;)
    activity_labels &lt;- activity_names$V1
    activity_levels &lt;- activity_names$V2

    # --Read the &#39;training&#39; dataset--
    df &lt;- readTable(&quot;train&quot;, &quot;subject&quot;, varNames=&quot;subject&quot;)
    df &lt;- cbind(df, readTable(&quot;train&quot;, &quot;X&quot;, varIndices=wanted_features_idx,
                      varNames=wanted_features))
    df &lt;- cbind(df, readTable(&quot;train&quot;, &quot;y&quot;, varNames=&quot;activity&quot;))
    df$activity &lt;- as.factor(df$activity); levels(df$activity) &lt;- activity_levels;
    df$dataset_type &lt;- &quot;train&quot;

    # --Now read the &#39;test&#39; dataset--
    df2 &lt;- readTable(&quot;test&quot;, &quot;subject&quot;, varNames=&quot;subject&quot;)
    df2 &lt;- cbind(df2, readTable(&quot;test&quot;, &quot;X&quot;, varIndices=wanted_features_idx,
                              varNames=wanted_features))
    df2 &lt;- cbind(df2, readTable(&quot;test&quot;, &quot;y&quot;, varNames=&quot;activity&quot;))
    df2$activity &lt;- as.factor(df2$activity); levels(df2$activity) &lt;- activity_levels;
    df2$dataset_type &lt;- &quot;test&quot;

    # *** RUBRIC #1: MERGE TRAINING AND TEST SETS INTO ONE ***
    df &lt;- rbind(df, df2)
    df$dataset_type &lt;- as.factor(df$dataset_type)

    df
}


df &lt;- readData()
</code></pre>

<pre><code>## Warning in if (!is.na(varIndices)) df &lt;- df[, varIndices]: the condition
## has length &gt; 1 and only the first element will be used
</code></pre>

<pre><code>## Warning in if (!is.na(varNames)) names(df) &lt;- varNames: the condition has
## length &gt; 1 and only the first element will be used
</code></pre>

<pre><code>## Warning in if (!is.na(varIndices)) df &lt;- df[, varIndices]: the condition
## has length &gt; 1 and only the first element will be used
</code></pre>

<pre><code>## Warning in if (!is.na(varNames)) names(df) &lt;- varNames: the condition has
## length &gt; 1 and only the first element will be used
</code></pre>

<pre><code class="r">n &lt;- NCOL(df)
str(df)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    10299 obs. of  89 variables:
##  $ subject                             : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ tBodyAcc_mean_X                     : num  0.289 0.278 0.28 0.279 0.277 ...
##  $ tBodyAcc_mean_Y                     : num  -0.0203 -0.0164 -0.0195 -0.0262 -0.0166 ...
##  $ tBodyAcc_mean_Z                     : num  -0.133 -0.124 -0.113 -0.123 -0.115 ...
##  $ tBodyAcc_std_X                      : num  -0.995 -0.998 -0.995 -0.996 -0.998 ...
##  $ tBodyAcc_std_Y                      : num  -0.983 -0.975 -0.967 -0.983 -0.981 ...
##  $ tBodyAcc_std_Z                      : num  -0.914 -0.96 -0.979 -0.991 -0.99 ...
##  $ tGravityAcc_mean_X                  : num  0.963 0.967 0.967 0.968 0.968 ...
##  $ tGravityAcc_mean_Y                  : num  -0.141 -0.142 -0.142 -0.144 -0.149 ...
##  $ tGravityAcc_mean_Z                  : num  0.1154 0.1094 0.1019 0.0999 0.0945 ...
##  $ tGravityAcc_std_X                   : num  -0.985 -0.997 -1 -0.997 -0.998 ...
##  $ tGravityAcc_std_Y                   : num  -0.982 -0.989 -0.993 -0.981 -0.988 ...
##  $ tGravityAcc_std_Z                   : num  -0.878 -0.932 -0.993 -0.978 -0.979 ...
##  $ tBodyAccJerk_mean_X                 : num  0.078 0.074 0.0736 0.0773 0.0734 ...
##  $ tBodyAccJerk_mean_Y                 : num  0.005 0.00577 0.0031 0.02006 0.01912 ...
##  $ tBodyAccJerk_mean_Z                 : num  -0.06783 0.02938 -0.00905 -0.00986 0.01678 ...
##  $ tBodyAccJerk_std_X                  : num  -0.994 -0.996 -0.991 -0.993 -0.996 ...
##  $ tBodyAccJerk_std_Y                  : num  -0.988 -0.981 -0.981 -0.988 -0.988 ...
##  $ tBodyAccJerk_std_Z                  : num  -0.994 -0.992 -0.99 -0.993 -0.992 ...
##  $ tBodyGyro_mean_X                    : num  -0.0061 -0.0161 -0.0317 -0.0434 -0.034 ...
##  $ tBodyGyro_mean_Y                    : num  -0.0314 -0.0839 -0.1023 -0.0914 -0.0747 ...
##  $ tBodyGyro_mean_Z                    : num  0.1077 0.1006 0.0961 0.0855 0.0774 ...
##  $ tBodyGyro_std_X                     : num  -0.985 -0.983 -0.976 -0.991 -0.985 ...
##  $ tBodyGyro_std_Y                     : num  -0.977 -0.989 -0.994 -0.992 -0.992 ...
##  $ tBodyGyro_std_Z                     : num  -0.992 -0.989 -0.986 -0.988 -0.987 ...
##  $ tBodyGyroJerk_mean_X                : num  -0.0992 -0.1105 -0.1085 -0.0912 -0.0908 ...
##  $ tBodyGyroJerk_mean_Y                : num  -0.0555 -0.0448 -0.0424 -0.0363 -0.0376 ...
##  $ tBodyGyroJerk_mean_Z                : num  -0.062 -0.0592 -0.0558 -0.0605 -0.0583 ...
##  $ tBodyGyroJerk_std_X                 : num  -0.992 -0.99 -0.988 -0.991 -0.991 ...
##  $ tBodyGyroJerk_std_Y                 : num  -0.993 -0.997 -0.996 -0.997 -0.996 ...
##  $ tBodyGyroJerk_std_Z                 : num  -0.992 -0.994 -0.992 -0.993 -0.995 ...
##  $ tBodyAccMag_mean_                   : num  -0.959 -0.979 -0.984 -0.987 -0.993 ...
##  $ tBodyAccMag_std_                    : num  -0.951 -0.976 -0.988 -0.986 -0.991 ...
##  $ tGravityAccMag_mean_                : num  -0.959 -0.979 -0.984 -0.987 -0.993 ...
##  $ tGravityAccMag_std_                 : num  -0.951 -0.976 -0.988 -0.986 -0.991 ...
##  $ tBodyAccJerkMag_mean_               : num  -0.993 -0.991 -0.989 -0.993 -0.993 ...
##  $ tBodyAccJerkMag_std_                : num  -0.994 -0.992 -0.99 -0.993 -0.996 ...
##  $ tBodyGyroMag_mean_                  : num  -0.969 -0.981 -0.976 -0.982 -0.985 ...
##  $ tBodyGyroMag_std_                   : num  -0.964 -0.984 -0.986 -0.987 -0.989 ...
##  $ tBodyGyroJerkMag_mean_              : num  -0.994 -0.995 -0.993 -0.996 -0.996 ...
##  $ tBodyGyroJerkMag_std_               : num  -0.991 -0.996 -0.995 -0.995 -0.995 ...
##  $ fBodyAcc_mean_X                     : num  -0.995 -0.997 -0.994 -0.995 -0.997 ...
##  $ fBodyAcc_mean_Y                     : num  -0.983 -0.977 -0.973 -0.984 -0.982 ...
##  $ fBodyAcc_mean_Z                     : num  -0.939 -0.974 -0.983 -0.991 -0.988 ...
##  $ fBodyAcc_std_X                      : num  -0.995 -0.999 -0.996 -0.996 -0.999 ...
##  $ fBodyAcc_std_Y                      : num  -0.983 -0.975 -0.966 -0.983 -0.98 ...
##  $ fBodyAcc_std_Z                      : num  -0.906 -0.955 -0.977 -0.99 -0.992 ...
##  $ fBodyAcc_meanFreq_X                 : num  0.252 0.271 0.125 0.029 0.181 ...
##  $ fBodyAcc_meanFreq_Y                 : num  0.1318 0.0429 -0.0646 0.0803 0.058 ...
##  $ fBodyAcc_meanFreq_Z                 : num  -0.0521 -0.0143 0.0827 0.1857 0.5598 ...
##  $ fBodyAccJerk_mean_X                 : num  -0.992 -0.995 -0.991 -0.994 -0.996 ...
##  $ fBodyAccJerk_mean_Y                 : num  -0.987 -0.981 -0.982 -0.989 -0.989 ...
##  $ fBodyAccJerk_mean_Z                 : num  -0.99 -0.99 -0.988 -0.991 -0.991 ...
##  $ fBodyAccJerk_std_X                  : num  -0.996 -0.997 -0.991 -0.991 -0.997 ...
##  $ fBodyAccJerk_std_Y                  : num  -0.991 -0.982 -0.981 -0.987 -0.989 ...
##  $ fBodyAccJerk_std_Z                  : num  -0.997 -0.993 -0.99 -0.994 -0.993 ...
##  $ fBodyAccJerk_meanFreq_X             : num  0.8704 0.6085 0.1154 0.0358 0.2734 ...
##  $ fBodyAccJerk_meanFreq_Y             : num  0.2107 -0.0537 -0.1934 -0.093 0.0791 ...
##  $ fBodyAccJerk_meanFreq_Z             : num  0.2637 0.0631 0.0383 0.1681 0.2924 ...
##  $ fBodyGyro_mean_X                    : num  -0.987 -0.977 -0.975 -0.987 -0.982 ...
##  $ fBodyGyro_mean_Y                    : num  -0.982 -0.993 -0.994 -0.994 -0.993 ...
##  $ fBodyGyro_mean_Z                    : num  -0.99 -0.99 -0.987 -0.987 -0.989 ...
##  $ fBodyGyro_std_X                     : num  -0.985 -0.985 -0.977 -0.993 -0.986 ...
##  $ fBodyGyro_std_Y                     : num  -0.974 -0.987 -0.993 -0.992 -0.992 ...
##  $ fBodyGyro_std_Z                     : num  -0.994 -0.99 -0.987 -0.989 -0.988 ...
##  $ fBodyGyro_meanFreq_X                : num  -0.2575 -0.0482 -0.2167 0.2169 -0.1533 ...
##  $ fBodyGyro_meanFreq_Y                : num  0.0979 -0.4016 -0.0173 -0.1352 -0.0884 ...
##  $ fBodyGyro_meanFreq_Z                : num  0.5472 -0.0682 -0.1107 -0.0497 -0.1622 ...
##  $ fBodyAccMag_mean_                   : num  -0.952 -0.981 -0.988 -0.988 -0.994 ...
##  $ fBodyAccMag_std_                    : num  -0.956 -0.976 -0.989 -0.987 -0.99 ...
##  $ fBodyAccMag_meanFreq_               : num  -0.0884 -0.0441 0.2579 0.0736 0.3943 ...
##  $ fBodyBodyAccJerkMag_mean_           : num  -0.994 -0.99 -0.989 -0.993 -0.996 ...
##  $ fBodyBodyAccJerkMag_std_            : num  -0.994 -0.992 -0.991 -0.992 -0.994 ...
##  $ fBodyBodyAccJerkMag_meanFreq_       : num  0.347 0.532 0.661 0.679 0.559 ...
##  $ fBodyBodyGyroMag_mean_              : num  -0.98 -0.988 -0.989 -0.989 -0.991 ...
##  $ fBodyBodyGyroMag_std_               : num  -0.961 -0.983 -0.986 -0.988 -0.989 ...
##  $ fBodyBodyGyroMag_meanFreq_          : num  -0.129 -0.272 -0.2127 -0.0357 -0.2736 ...
##  $ fBodyBodyGyroJerkMag_mean_          : num  -0.992 -0.996 -0.995 -0.995 -0.995 ...
##  $ fBodyBodyGyroJerkMag_std_           : num  -0.991 -0.996 -0.995 -0.995 -0.995 ...
##  $ fBodyBodyGyroJerkMag_meanFreq_      : num  -0.0743 0.1581 0.4145 0.4046 0.0878 ...
##  $ angle_tBodyAccMean_gravity_         : num  -0.1128 0.0535 -0.1186 -0.0368 0.1233 ...
##  $ angle_tBodyAccJerkMean_gravityMean_ : num  0.0304 -0.00743 0.1779 -0.01289 0.12254 ...
##  $ angle_tBodyGyroMean_gravityMean_    : num  -0.465 -0.733 0.101 0.64 0.694 ...
##  $ angle_tBodyGyroJerkMean_gravityMean_: num  -0.0184 0.7035 0.8085 -0.4854 -0.616 ...
##  $ angle_X_gravityMean_                : num  -0.841 -0.845 -0.849 -0.849 -0.848 ...
##  $ angle_Y_gravityMean_                : num  0.18 0.18 0.181 0.182 0.185 ...
##  $ angle_Z_gravityMean_                : num  -0.0586 -0.0543 -0.0491 -0.0477 -0.0439 ...
##  $ activity                            : Factor w/ 6 levels &quot;WALKING&quot;,&quot;WALKING_UPSTAIRS&quot;,..: 5 5 5 5 5 5 5 5 5 5 ...
##  $ dataset_type                        : Factor w/ 2 levels &quot;test&quot;,&quot;train&quot;: 2 2 2 2 2 2 2 2 2 2 ...
</code></pre>

<pre><code class="r"># The 3 variables (subject, activity, dataset_type) that were santized much
summary(df[,c(1, n-1, n)])
</code></pre>

<pre><code>##     subject                    activity    dataset_type
##  Min.   : 1.00   WALKING           :1722   test :2947  
##  1st Qu.: 9.00   WALKING_UPSTAIRS  :1544   train:7352  
##  Median :17.00   WALKING_DOWNSTAIRS:1406               
##  Mean   :16.15   SITTING           :1777               
##  3rd Qu.:24.00   STANDING          :1906               
##  Max.   :30.00   LAYING            :1944
</code></pre>

<pre><code class="r"># *** RUBRIC #5: FROM THE DATASET IN STEP4, CREATE A SECOND, INDEPENDENT
#     TIDY DATASET WITH AVG OF EACH VAR FOR EACH ACTIVITY, FOR EACH SUBJECT ***
library(dplyr)
tidy_df &lt;- df %&gt;% group_by(subject, activity) %&gt;% 
             select(2:n-2) %&gt;% summarise_each(funs(mean))
n &lt;- NCOL(tidy_df)
summary(tidy_df[,-c(5:80)])  # Just a few first columns to visually check
</code></pre>

<pre><code>##     subject                   activity  tBodyAcc_mean_X 
##  Min.   : 1.0   WALKING           :30   Min.   :0.2216  
##  1st Qu.: 8.0   WALKING_UPSTAIRS  :30   1st Qu.:0.2712  
##  Median :15.5   WALKING_DOWNSTAIRS:30   Median :0.2770  
##  Mean   :15.5   SITTING           :30   Mean   :0.2743  
##  3rd Qu.:23.0   STANDING          :30   3rd Qu.:0.2800  
##  Max.   :30.0   LAYING            :30   Max.   :0.3015  
##  tBodyAcc_mean_Y     fBodyBodyGyroJerkMag_meanFreq_
##  Min.   :-0.040514   Min.   :-0.18292              
##  1st Qu.:-0.020022   1st Qu.: 0.05423              
##  Median :-0.017262   Median : 0.11156              
##  Mean   :-0.017876   Mean   : 0.12592              
##  3rd Qu.:-0.014936   3rd Qu.: 0.20805              
##  Max.   :-0.001308   Max.   : 0.42630              
##  angle_tBodyAccMean_gravity_ angle_tBodyAccJerkMean_gravityMean_
##  Min.   :-0.163043           Min.   :-0.1205540                 
##  1st Qu.:-0.011012           1st Qu.:-0.0211694                 
##  Median : 0.007878           Median : 0.0031358                 
##  Mean   : 0.006556           Mean   : 0.0006439                 
##  3rd Qu.: 0.024393           3rd Qu.: 0.0220881                 
##  Max.   : 0.129154           Max.   : 0.2032600                 
##  angle_tBodyGyroMean_gravityMean_ angle_tBodyGyroJerkMean_gravityMean_
##  Min.   :-0.38931                 Min.   :-0.22367                    
##  1st Qu.:-0.01977                 1st Qu.:-0.05613                    
##  Median : 0.02087                 Median :-0.01602                    
##  Mean   : 0.02193                 Mean   :-0.01137                    
##  3rd Qu.: 0.06460                 3rd Qu.: 0.03200                    
##  Max.   : 0.44410                 Max.   : 0.18238                    
##  angle_X_gravityMean_ angle_Y_gravityMean_ angle_Z_gravityMean_
##  Min.   :-0.9471      Min.   :-0.87457     Min.   :-0.873649   
##  1st Qu.:-0.7907      1st Qu.: 0.02191     1st Qu.:-0.083912   
##  Median :-0.7377      Median : 0.17136     Median : 0.005079   
##  Mean   :-0.5243      Mean   : 0.07865     Mean   :-0.040436   
##  3rd Qu.:-0.5823      3rd Qu.: 0.24343     3rd Qu.: 0.106190   
##  Max.   : 0.7378      Max.   : 0.42476     Max.   : 0.390444
</code></pre>

<pre><code class="r">write.table(tidy_df, file = &quot;getdata_peerassign2_tidy_data.txt&quot;, row.names = FALSE)
</code></pre>

<p>For details about the data transformations done please look at the CodeBook.md file.</p>

</body>

</html>
