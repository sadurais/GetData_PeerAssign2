# GetData_PeerAssign2
Coursera Data Science Speclzn - Getting and Cleaning Data - Peer Assignment 2 

# SYNOPSIS
The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. You will be graded by your peers on a series of yes/no questions related to the project. You will be required to submit: 1) a tidy data set as described below, 2) a link to a Github repository with your script for performing the analysis, and 3) a code book that describes the variables, the data, and any transformations or work that you performed to clean up the data called CodeBook.md. You should also include a README.md in the repo with your scripts. This repo explains how all of the scripts work and how they are connected.  

One of the most exciting areas in all of data science right now is wearable computing - see for example this article . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained: 


# APPROACH
   Once the zip file 'getdata-projectfiles-UCI HAR Dataset.zip' is unzipped, we see the folowing directory structure inside. 
```{r echo=TRUE, eval=FALSE}
    $ tree "UCI HAR Dataset"
    UCI HAR Dataset
    ├── test/
    │   ├── Inertial Signals/
    │   ├── X_test.txt
    │   ├── subject_test.txt
    │   └── y_test.txt
    ├── train/
    │   ├── Inertial Signals/
    │   ├── X_train.txt
    │   ├── subject_train.txt
    │   └── y_train.txt
    ├── README.txt
    ├── activity_labels.txt
    ├── features.txt
    └── features_info.txt
```
We can see that the subdirectories 'train' and 'test' have similar structure and data files. We can ignore the Inertial Signals (as per Course Community TA) and focus on the 3 data files 'X', 'subject', and 'y': the 'feature vector values', 'the test subject from whom the device is collecting the activity data', and 'the activity' itself respectively. A quick cursory look at these files show us that these files are space-separated, do not have header lines or quoted values or comment lines, making read.table suitable for reading. Also the 'X' data is 561 variable (columns) data, the entirety of which will not be useful for us, but only the few variables that have to do 'mean' and 'standard deviation' (that is, those that have 'mean' or 'std' in their name, case in-sensitive). 
We also notice that, at the root directory, we have meta-data files such as 'activity_labels.txt' and 'features.txt' giving meaning to the dataset files inside the 'train' and 'test' subdirectories. 'activity_labels.txt' lists the human understandable labels of the SIX activities: WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING and the features.txt gives the labels for the 561 variables.




## The complete code is listed below:
```{r echo=TRUE, eval=TRUE}
#
# Getting and Cleaning Data - Peer Assignment 2
#
# 1) Merge the training and test sets to create one data set
# 2) Extract only the measurements on the mean and standard-deviation 
#   for each measurement
# 3) Use descriptive activity names to name the activities in the data set
# 4) Appropriately lable the data set with descriptive variable names
# 5) From the data set in step 4, create a second, independent tidy data set 
#   with average of each variable for each activity for each subject
#

library(dplyr)
library(data.table)


#Handy function to construct and return the filepath
filePath <- function(datasetType="train", fileNamePrefix="y") {
    directory <- "UCI HAR Dataset"
    path <- paste(directory, "/", datasetType, "/",
                fileNamePrefix, "_", datasetType, ".txt", sep="")
    path
}


# Handy function to keep read.table params, setting varNames
# and subsetting variables(columns) in one place.
readTable <- function(datasetType="train", fileNamePrefix="y",
                      varIndices=NA, varNames=NA) {
    df = read.table(filePath(datasetType, fileNamePrefix), header=FALSE,
               sep="", quote="", comment.char="",
               nrow=7400, colClasses=c("numeric"))
    if (!is.na(varIndices)) df <- df[, varIndices]
    if (!is.na(varNames)) names(df) <- varNames
    df
}


# Similar to readTable, but reads the meta-data files such as
# features.txt and activity_labels.txt rather than a data file.
readMetaTable <- function(fileName) {
    directory <- "UCI HAR Dataset"
    filePath <- paste(directory, "/", fileName, sep="")
    read.table(filePath, header=FALSE, sep="", quote="",
               comment.char="")
}


readData <- function() {

    # --First, lets read meta-data about features and activity-labels--
    # Read the meaningful names of feature variables. There are some
    # duplicate names. Fortunately none of them have 'mean' or 'std' in
    # their name that we are interested in. So, we can safely ignore them
    # *according to a community TA*.
    feature_names <- readMetaTable("features.txt")

    # *** RUBRIC #2: EXTRACT ONLY MEAN & STDDEV FOR EACH MEASUREMENT ***
    wanted_features_idx <- grep("mean|std", feature_names$V2, ignore.case = TRUE)
    wanted_features <- feature_names$V2[wanted_features_idx]

    # *** RUBRIC #4: APPROPRIATELY NAME THE DATASET WITH DESCRIPTIVE VAR NAMES ***
    # Non-word characters such as '-', '(', ')' can create problems in R
    # as variable-names. So, lets replace those chars with '__'
    wanted_features <- gsub("\\W+", "_", wanted_features)

    # *** RUBRIC #3: USE DESCRIPTIVE ACTIVITY NAMES ***
    # Read the discrete labels of outcome (aka "y" or "activity")
    activity_names <- readMetaTable("activity_labels.txt")
    activity_labels <- activity_names$V1
    activity_levels <- activity_names$V2


    # --Read the 'training' dataset--
    df <- readTable("train", "subject", varNames="subject")
    df <- cbind(df, readTable("train", "X", varIndices=wanted_features_idx,
                      varNames=wanted_features))
    df <- cbind(df, readTable("train", "y", varNames="activity"))
    df$activity <- as.factor(df$activity); levels(df$activity) <- activity_levels;
    df$dataset_type <- "train"


    # --Now read the 'test' dataset--
    df2 <- readTable("test", "subject", varNames="subject")
    df2 <- cbind(df2, readTable("test", "X", varIndices=wanted_features_idx,
                              varNames=wanted_features))
    df2 <- cbind(df2, readTable("test", "y", varNames="activity"))
    df2$activity <- as.factor(df2$activity); levels(df2$activity) <- activity_levels;
    df2$dataset_type <- "test"


    # *** RUBRIC #1: MERGE TRAINING AND TEST SETS INTO ONE ***
    df <- rbind(df, df2)
    df$dataset_type <- as.factor(df$dataset_type)

    df
}


df <- readData()
n <- NCOL(df)
str(df)

# The 3 variables (subject, activity, dataset_type) that were santized much
summary(df[,c(1, n-1, n)])


# *** RUBRIC #5: FROM THE DATASET IN STEP4, CREATE A SECOND, INDEPENDENT
#     TIDY DATASET WITH AVG OF EACH VAR FOR EACH ACTIVITY, FOR EACH SUBJECT ***
library(dplyr)
tidy_df <- df %>% group_by(subject, activity) %>% 
             select(2:n-2) %>% summarise_each(funs(mean))
n <- NCOL(tidy_df)
summary(tidy_df[,-c(5:80)])  # Just a few first columns to visually check

write.table(tidy_df, file = "getdata_peerassign2_tidy_data.txt", row.names = FALSE)
```