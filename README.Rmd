# GetData_PeerAssign2
Coursera Data Science Speclzn - Getting and Cleaning Data - Peer Assignment 2 

## SYNOPSIS
The goal of this project is to collect, work with, and clean a data set to prepare a filtered, tidy data that can be used for later analysis. The dataset is about wearable computing. Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data that we have represent data collected from the accelerometers from the Samsung Galaxy S smartphone. We read, process and transform the data into a tidy dataset and store it another file for further analysis.


## APPROACH
We see the following structure inside out dataset zip file 'getdata-projectfiles-UCI HAR Dataset.zip'. 
```{r echo=TRUE, eval=FALSE}
    $ tree "UCI HAR Dataset"
    UCI HAR Dataset
    ├── test/
    │   ├── Inertial Signals/
    │   ├── X_test.txt
    │   ├── subject_test.txt
    │   └── y_test.txt
    ├── train/
    │   ├── Inertial Signals/
    │   ├── X_train.txt
    │   ├── subject_train.txt
    │   └── y_train.txt
    ├── README.txt
    ├── activity_labels.txt
    ├── features.txt
    └── features_info.txt
```
We can see that the subdirectories 'train' and 'test' have similar structure and data files. We focus on the 3 data files 'X', 'subject', and 'y': the 'feature vector values', 'the test subject from whom the device is collecting the activity data', and 'the activity' itself respectively. A quick cursory look at these files show us that these files are space-separated, do not have header lines or quoted values or comment lines, making read.table suitable for reading. Also the 'X' data is 561 variable (columns) data, the entirety of which will not be useful for us, but only the few variables that have to do 'mean' and 'standard deviation' (that is, those that have 'mean' or 'std' in their name, case in-sensitive). 
We also notice that, at the root directory, we have meta-data files such as 'activity_labels.txt' and 'features.txt' giving meaning to the dataset files inside the 'train' and 'test' subdirectories. 'activity_labels.txt' lists the human understandable labels of the SIX activities: WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING and the features.txt gives the labels for the 561 variables.


## Source Credtis
  *For more and thorough information about the origin/ideas of this dataset, please see* [this link][1]
  [1]: http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
  


## Transformations done to the data:
  - *Subsetting the features*: Even though there are 561 variables measured
       from the sensors, we need only those that measure the 'mean'
       and 'standard deviation'. So we grep for those patterns 'mean|std' in
       the names of the features that we obtain from the features.txt file.
  - *Attach feature-names to the feature data*: The feature names and feature
       data are conveyed in two different files: features.txt and
       {train|test}/X_{train|test}.txt files. We attach the names to the data
       in our data frame using names(df) <- features_names_vect
  - *Sanitizing the feature labels/names*: The feature names from features.txt
       file have non-word characters such as '-', '(', ')' that could cause
       problems with certain R functions. We find&replace such chars with
       a '_' using regex:  s/\W+/_/g  (global replace a sequence of 1 or more
       non-word characters into a single underscore).So, "tBodyAcc-arCoeff()-X,1"
       becomes "tBodyAcc_arCoeff_X_1"
  - *Ensuring categorical variables are discrete*: Variables with few discrete
       values needs to be treated as factors for our further analysis to work
       nicely. We use 'as.factor' to convert these variables (activity & dataset_type)
  - *Combine training and test datasets into one*: The subdirectories 'train'
       and 'test' have similar structure. We use modularized code to read them
       alike and combine them into one data frame using:
       df <- training-set's data
       df <- cbind(df, testing-set's data)
  - *Grouping & Summarizing our data*: We first group by test-subject, then the
       activity and finally take the mean of all the other variables (with index
       2:ncol-2). With these variables we create our final tidy dataset. This
       second dataset is stored in a file: getdata_peerassign2_tidy_data.txt
       as space separated values, for quicker reading by read.table

## Transformation steps
  First we devlop few handy functions:
```{r echo=TRUE, eval=FALSE}
#Handy function to construct and return the filepath
filePath <- function(datasetType="train", fileNamePrefix="y") {
    directory <- "UCI HAR Dataset"
    path <- paste(directory, "/", datasetType, "/",
                fileNamePrefix, "_", datasetType, ".txt", sep="")
    path
}
```

```{r echo=TRUE, eval=FALSE}
# Handy function to keep read.table params, setting varNames
# and subsetting variables(columns) in one place.
readTable <- function(datasetType="train", fileNamePrefix="y",
                      varIndices=NA, varNames=NA) {
    df = read.table(filePath(datasetType, fileNamePrefix), header=FALSE,
               sep="", quote="", comment.char="",
               nrow=7400, colClasses=c("numeric"))
    if (!is.na(varIndices)) df <- df[, varIndices]
    if (!is.na(varNames)) names(df) <- varNames
    df
}
```
If variable indices are passed, this function would subset the data frame 
to have only those variables(columns).  If variable names are passed,
it assigns them to the variables(columns).


```{r echo=TRUE, eval=FALSE}
# Similar to readTable, but reads the meta-data files such as
# features.txt and activity_labels.txt rather than a data file.
readMetaTable <- function(fileName) {
    directory <- "UCI HAR Dataset"
    filePath <- paste(directory, "/", fileName, sep="")
    read.table(filePath, header=FALSE, sep="", quote="",
               comment.char="")
}
```


## The complete code (with annotations to the RUBRICs) is listed below:
```{r echo=TRUE, eval=TRUE, cache=TRUE}
#
# Getting and Cleaning Data - Peer Assignment 2
#
# 1) Merge the training and test sets to create one data set
# 2) Extract only the measurements on the mean and standard-deviation 
#   for each measurement
# 3) Use descriptive activity names to name the activities in the data set
# 4) Appropriately lable the data set with descriptive variable names
# 5) From the data set in step 4, create a second, independent tidy data set 
#   with average of each variable for each activity for each subject
#

library(dplyr)
library(data.table)


#Handy function to construct and return the filepath
filePath <- function(datasetType="train", fileNamePrefix="y") {
    directory <- "UCI HAR Dataset"
    path <- paste(directory, "/", datasetType, "/",
                fileNamePrefix, "_", datasetType, ".txt", sep="")
    path
}


# Handy function to keep read.table params, setting varNames
# and subsetting variables(columns) in one place.
readTable <- function(datasetType="train", fileNamePrefix="y",
                      varIndices=NA, varNames=NA) {
    df = read.table(filePath(datasetType, fileNamePrefix), header=FALSE,
               sep="", quote="", comment.char="",
               nrow=7400, colClasses=c("numeric"))
    if (!is.na(varIndices)) df <- df[, varIndices]
    if (!is.na(varNames)) names(df) <- varNames
    df
}


# Similar to readTable, but reads the meta-data files such as
# features.txt and activity_labels.txt rather than a data file.
readMetaTable <- function(fileName) {
    directory <- "UCI HAR Dataset"
    filePath <- paste(directory, "/", fileName, sep="")
    read.table(filePath, header=FALSE, sep="", quote="",
               comment.char="")
}


readData <- function() {

    # --First, lets read meta-data about features and activity-labels--
    # Read the meaningful names of feature variables. There are some
    # duplicate names. Fortunately none of them have 'mean' or 'std' in
    # their name that we are interested in. So, we can safely ignore them
    # *according to a community TA*.
    feature_names <- readMetaTable("features.txt")

    # *** RUBRIC #2: EXTRACT ONLY MEAN & STDDEV FOR EACH MEASUREMENT ***
    wanted_features_idx <- grep("mean|std", feature_names$V2, ignore.case = TRUE)
    wanted_features <- feature_names$V2[wanted_features_idx]

    # *** RUBRIC #4: APPROPRIATELY NAME THE DATASET WITH DESCRIPTIVE VAR NAMES ***
    # Non-word characters such as '-', '(', ')' can create problems in R
    # as variable-names. So, lets replace those chars with '__'
    wanted_features <- gsub("\\W+", "_", wanted_features)

    # *** RUBRIC #3: USE DESCRIPTIVE ACTIVITY NAMES ***
    # Read the discrete labels of outcome (aka "y" or "activity")
    activity_names <- readMetaTable("activity_labels.txt")
    activity_labels <- activity_names$V1
    activity_levels <- activity_names$V2


    # --Read the 'training' dataset--
    df <- readTable("train", "subject", varNames="subject")
    df <- cbind(df, readTable("train", "X", varIndices=wanted_features_idx,
                      varNames=wanted_features))
    df <- cbind(df, readTable("train", "y", varNames="activity"))
    df$activity <- as.factor(df$activity); levels(df$activity) <- activity_levels;
    df$dataset_type <- "train"


    # --Now read the 'test' dataset--
    df2 <- readTable("test", "subject", varNames="subject")
    df2 <- cbind(df2, readTable("test", "X", varIndices=wanted_features_idx,
                              varNames=wanted_features))
    df2 <- cbind(df2, readTable("test", "y", varNames="activity"))
    df2$activity <- as.factor(df2$activity); levels(df2$activity) <- activity_levels;
    df2$dataset_type <- "test"


    # *** RUBRIC #1: MERGE TRAINING AND TEST SETS INTO ONE ***
    df <- rbind(df, df2)
    df$dataset_type <- as.factor(df$dataset_type)

    df
}


df <- readData()
n <- NCOL(df)
str(df)

# The 3 variables (subject, activity, dataset_type) that were santized much
summary(df[,c(1, n-1, n)])


# *** RUBRIC #5: FROM THE DATASET IN STEP4, CREATE A SECOND, INDEPENDENT
#     TIDY DATASET WITH AVG OF EACH VAR FOR EACH ACTIVITY, FOR EACH SUBJECT ***
library(dplyr)
tidy_df <- df %>% group_by(subject, activity) %>% 
             select(2:n-2) %>% summarise_each(funs(mean))
n <- NCOL(tidy_df)
summary(tidy_df[,-c(5:80)])  # Just a few first columns to visually check

write.table(tidy_df, file = "getdata_peerassign2_tidy_data.txt", row.names = FALSE)
```

For details about the data transformations done please look at the CodeBook.md file.
